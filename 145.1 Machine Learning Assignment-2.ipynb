{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1: Define overfitting and underfitting in machine learning. What are the consequences of each, and how\n",
    "can they be mitigated?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "overfitting- Overfitting is an undesirable machine learning behavior that occurs when the machine learning model gives accurate predictions for training data but not for test/new data. Basically training accuracy is high but test accuracy is low.\n",
    "\n",
    "An overfit model can give inaccurate predictions and cannot perform well for all types of new data.\n",
    "\n",
    "Below are possible options to prevent overfitting, which helps improve the model performance.\n",
    "Training with more data.\n",
    "Data augmentation.\n",
    "Addition of noise to the input data\n",
    "Feature selection.\n",
    "Cross-validation.\n",
    "Simplify data.\n",
    "Regularization.\n",
    "Ensembling.\n",
    "\n",
    "Underfitting- Underfitting is a scenario in data science where a data model is unable to capture the relationship between the input and output variables accurately, generating a high error rate on both the training set and unseen data. Basically training accuracy is low and test accuracy is also low.\n",
    "\n",
    "An underfit model has poor performance on the training data and will result in unreliable predictions.\n",
    "\n",
    "Like overfitting, when a model is underfitted, it cannot establish the dominant trend within the data, resulting in training errors and poor performance of the model. If a model cannot generalize well to new data, then it cannot be leveraged for classification or prediction tasks.\n",
    "\n",
    "We can solve the problem of overfitting by: \n",
    "Increasing the training data by data augmentation. \n",
    "\n",
    "Feature selection by choosing the best features and remove the useless/unnecessary features. \n",
    "\n",
    "Early stopping the training of deep learning models where the number of epochs is set high.\n",
    "\n",
    "regularization technique is used for reducing model overfitting by reducing model complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2: How can we reduce overfitting? Explain in brief."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ways to Tackle Underfitting\n",
    "\n",
    "Increase the number of features in the dataset.\n",
    "\n",
    "Increase model complexity.\n",
    "\n",
    "Reduce noise in the data.\n",
    "\n",
    "Increase the duration of training the data.\n",
    "\n",
    "Early stopping. Early stopping pauses the training phase before the machine learning model learns the noise in the data\n",
    "\n",
    "pruning—identifies the most important features within the training set and eliminates irrelevant one\n",
    "\n",
    "Regularization- These methods try to eliminate those factors that do not impact the prediction outcomes by grading features based on importance\n",
    "\n",
    "Data augmentation- When done in moderation, data augmentation makes the training sets appear unique to the model and prevents the model from learning their characteristics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3: Explain underfitting. List scenarios where underfitting can occur in ML."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Underfitting is a scenario in data science where a data model is unable to capture the relationship between the input and output variables accurately, generating a high error rate on both the training set and unseen data.It usually happens when we have less data to build an accurate model and also when we try to build a linear model with fewer non-linear data\n",
    "\n",
    "Underfitting occurs when the model is too simple or when there is not enough training data to capture the true complexity of the problem. One common example of underfitting is when we use a linear model to fit a dataset that has a non-linear relationship between the input and output variables.\n",
    "\n",
    "Underfitting occurs when a model is too simple — informed by too few features or regularized too much — which makes it inflexible in learning from the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4: Explain the bias-variance tradeoff in machine learning. What is the relationship between bias and\n",
    "variance, and how do they affect model performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bias-variance trade-off is tension between the error introduced by the bias(system errors) and the error produced by the variance(random errors).\n",
    "\n",
    "Bias and variance are inversely connected. It is impossible to have an ML model with a low bias and a low variance. \n",
    "\n",
    "A linear machine-learning algorithm will exhibit high bias but low variance. On the other hand, a non-linear algorithm will exhibit low bias but high variance. Using a linear model with a data set that is non-linear will introduce bias into the model.\n",
    "\n",
    "In machine learning, as we try to minimize one component of the error (e.g., bias), the other component (e.g., variance) tends to increase, and vice versa. Finding the right balance of bias and variance is key to creating an effective and accurate model. This is called the bias-variance tradeoff.\n",
    "\n",
    "If the algorithm is too simple (hypothesis with linear equation) then it may be on high bias and low variance condition and thus is error-prone. If algorithms fit too complex (hypothesis with high degree equation) then it may be on high variance and low bias. In the latter condition, the new entries will not perform well. Well, there is something between both of these conditions, known as a Trade-off or Bias Variance Trade-off. This tradeoff in complexity is why there is a tradeoff between bias and variance. An algorithm can’t be more complex and less complex at the same time.\n",
    "\n",
    "Bias is one type of error that occurs due to wrong assumptions about data such as assuming data is linear when in reality, data follows a complex function. On the other hand, variance gets introduced with high sensitivity to variations in training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5: Discuss some common methods for detecting overfitting and underfitting in machine learning models.\n",
    "How can you determine whether your model is overfitting or underfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An overfitting model performs well on the training data but doesn't generalize to testing data.An underfitting model performs poorly on training and testing data.\n",
    "\n",
    "We can determine whether a predictive model is underfitting or overfitting the training data by looking at the prediction error on the training data and the evaluation data. Our model is underfitting the training data when the model performs poorly on the training data.\n",
    "\n",
    "Another way of visualising performance is with a learning curve. This plot uses different size samples to perform the training. If there is a large gap between the train and validation score, then we are overfitting. If the training score is low, we are underfitting.\n",
    "\n",
    "We need to check the accuracy difference between train and test set for each fold result. If the model gives high training accuracy but low test accuracy so our model is overfitting. If the model does not give good training accuracy we can say our model is underfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q6: Compare and contrast bias and variance in machine learning. What are some examples of high bias\n",
    "and high variance models, and how do they differ in terms of their performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A model with high variance may represent the data set accurately but could lead to overfitting to noisy or otherwise unrepresentative training data. In comparison, a model with high bias may underfit the training data due to a simpler model that overlooks regularities in the data.\n",
    "\n",
    "Bias is one type of error that occurs due to wrong assumptions about data such as assuming data is linear when in reality, data follows a complex function. On the other hand, variance gets introduced with high sensitivity to variations in training data.\n",
    "\n",
    "A linear machine-learning algorithm will exhibit high bias but low variance. On the other hand, a non-linear algorithm will exhibit low bias but high variance. Using a linear model with a data set that is non-linear will introduce bias into the model. The model will underfit the target functions compared to the training data set. The reverse is true as well — if you use a non-linear model on a linear dataset, the non-linear model will overfit the target function.\n",
    "\n",
    "Bias refers to the difference between predicted values and actual values. Variance says about how much a random variable deviates from its expected value.\n",
    "\n",
    "Examples of high-bias machine learning algorithms include: Linear Regression, Linear Discriminant Analysis and Logistic Regression.\n",
    "\n",
    "Examples of high-variance machine learning algorithms include: Decision trees, support vector machines and k-nearest neighbors.\n",
    "\n",
    "Bias and variance are inversely connected. It is impossible to have an ML model with a low bias and a low variance. When a data engineer modifies the ML algorithm to better fit a given data set, it will lead to low bias—but it will increase variance.\n",
    "\n",
    "In machine learning, as you try to minimize one component of the error (e.g., bias), the other component (e.g., variance) tends to increase, and vice versa. Finding the right balance of bias and variance is key to creating an effective and accurate model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q7: What is regularization in machine learning, and how can it be used to prevent overfitting? Describe\n",
    "some common regularization techniques and how they work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization refers to techniques that are used to calibrate machine learning models in order to minimize the adjusted loss function and prevent overfitting or underfitting.\n",
    "\n",
    "Regularization is a technique used to reduce errors by fitting the function appropriately on the given training set and avoiding overfitting. In short, Regularization in machine learning is the process of regularizing the parameters that constrain, regularizes, or shrinks the coefficient estimates towards zero. In other words, this technique discourages learning a more complex or flexible model, avoiding the risk of Overfitting. Using Regularization, we can fit our machine learning model appropriately on a given test set and hence reduce the errors in it.\n",
    "\n",
    "It involves adding a regularization term to the loss function, which penalizes large weights or complex model architectures. Regularization methods such as L1 and L2 regularization, dropout, and batch normalization help control model complexity and improve its ability to generalize to unseen data.\n",
    "\n",
    "There are two main regularization techniques, namely: Ridge Regression (L2 Norm), Lasso Regression (L1 Norm) \n",
    "\n",
    "L1 regularization, also known as L1 norm or Lasso (in regression problems), combats overfitting by shrinking the parameters towards 0. L1 regularization is more robust than L2 regularization for a fairly obvious reason. L2 regularization takes the square of the weights, so the cost of outliers present in the data increases exponentially. L1 regularization takes the absolute values of the weights, so the cost only increases linearly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
