{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its\n",
    "application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling the data can help to balance the impact of all variables on the distance calculation and can help to improve the performance of the algorithm.\n",
    "\n",
    "Variables that are measured at different scales do not contribute equally to the model fitting & model learned function and might end up creating a bias. Thus, to deal with this potential problem feature-wise normalization such as MinMax Scaling is usually used prior to model fitting.\n",
    "\n",
    "Rescaling (min-max normalization)\n",
    "Also known as min-max scaling or min-max normalization, rescaling is the simplest method and consists in rescaling the range of features to scale the range in [0, 1] \n",
    "\n",
    "Example:\n",
    "For example, for a dataset, we could guesstimate the min and max observable values as 30 and -10. We can then normalize any value, like 18.8, as follows:\n",
    "\n",
    "y = (x — min) / (max-min)\n",
    "\n",
    "\n",
    "y = (18.8 — (-10)) / (30 — (-10))\n",
    "\n",
    "y = 28.8 / 40\n",
    "\n",
    "y = 0.72\n",
    "\n",
    "the normalized values present between [0,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling?\n",
    "Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sccaling is done considering the whole feature vector to be of unit length.\n",
    "\n",
    "Unit vector scaling means dividing each component by the Euclidean length of the vector (L2 Norm).\n",
    "\n",
    "Like Min-Max Scaling, the Unit Vector technique produces values of range [0,1]. When dealing with features with hard boundaries, this is quite useful. For example, when dealing with image data, the colors can range from only 0 to 255.\n",
    "\n",
    "Unit vectors can also be scaled to get a new vector. Basically, the unit vector will be multiplied by a scalar quantity which will increase the magnitude of the vector, however, the direction will remain unaffected. That is the beauty of the scaling vectors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an\n",
    "example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principal component analysis (PCA) is a technique that transforms high-dimensions data into lower-dimensions while retaining as much information as possible keeping maximum variance. \n",
    "\n",
    "PCA can reduce the complexity and noise of the data, and highlight the most important features and relationships. For example, we can use PCA to visualize the similarities and differences between different groups of customers, products, or genes, based on multiple attributes.\n",
    "\n",
    "PCA works by considering the variance of each attribute because the high attribute shows the good split between the classes, and hence it reduces the dimensionality. Some real-world applications of PCA are image processing, movie recommendation system, optimizing the power allocation in various communication channels.\n",
    "\n",
    "Principal Component Analysis (PCA) is used to reduce the dimensionality of a data set by finding a new set of variables, smaller than the original set of variables, retaining most of the sample's information, and useful for the regression and classification of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature\n",
    "Extraction? Provide an example to illustrate this concept."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principal component analysis (PCA) is an unsupervised linear transformation technique which is primarily used for feature extraction and dimensionality reduction.\n",
    "\n",
    "PCA can be used to identify the most important features in a dataset, which can be used to build predictive models. PCA can be used to visualize high-dimensional data in two or three dimensions, making it easier to understand and interpret.\n",
    "The difference is that PCA will try to reduce dimensionality by exploring how one feature of the data is expressed in terms of the other features (linear dependecy). Feature selection instead, takes the target into consideration.\n",
    "\n",
    "PCA is a way of finding the most important features in a dataset. For example, if you have a dataset of pictures of dogs, PCA could find the features that make a dog look like a dog, such as its shape, size, and color.\n",
    "\n",
    "PCA for Feature Extraction (STEPS):\n",
    "1. Standardize the dataset\n",
    "2. Construct the covariance matrix\n",
    "3. Perform Eigendecomposition of covariance matrix\n",
    "4. Selection of most important Eigenvectors / Eigenvalues\n",
    "5. Projection matrix creation of important eigenvectors\n",
    "6. Training / test dataset transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset\n",
    "contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to\n",
    "preprocess the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Min-Max scaling is a normalization technique that enables us to scale data in a dataset to a specific range using each feature's minimum and maximum value.\n",
    "since we have 3 features in the dataset:\n",
    "\n",
    "A Min-Max scaling is typically done via the following equation: \n",
    "\n",
    "Xsc=X−Xmin/(Xmax−Xmin)\n",
    "\n",
    "code to preprocess using kin max scalar:\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "min_max= MinMaxScaler()\n",
    "\n",
    "min_max.fit(df[[\"price\",\"rating\",\"delivery time\"]])\n",
    "\n",
    "min_max.transform(df[[\"price\",\"rating\",\"delivery time\"]])\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q6. You are working on a project to build a model to predict stock prices. The dataset contains many\n",
    "features, such as company financial data and market trends. Explain how you would use PCA to reduce the\n",
    "dimensionality of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "steps how to use PCA to reduce the dimensionality of the dataset:\n",
    "\n",
    "First we transform each column of the dataset using standardscaler\n",
    "\n",
    "TRhen we fit PCA model on the transformed data (specifying the no. of eigen vectors or the no. of final transformed vectors required)\n",
    "\n",
    "After we get the eigenvectors we check for how much variance is explained by the no. of vectors used\n",
    "\n",
    "Then we increase the no. of eigenvectors by which we will be able to explain 90% of variance accordingly we increase the final eigenvectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the\n",
    "values to a range of -1 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rklab\\anaconda3\\lib\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but MinMaxScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.        ],\n",
       "       [0.21052632],\n",
       "       [0.47368421],\n",
       "       [0.73684211],\n",
       "       [1.        ]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "df= pd.DataFrame(x,columns=['actual_list'])\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(df['actual_list'].values.reshape(-1,1))\n",
    "scaler.transform(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
